{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from PIL import ImageOps \n",
    "import numpy as np \n",
    "import os \n",
    "\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imageToPixels import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate() \n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def load_image2(infilename) :\n",
    "#     '''  \n",
    "#     convert image file to pixels\n",
    "#     load_image2('fileName')\n",
    "#     '''\n",
    "#     img = Image.open(infilename).convert('L')\n",
    "#     data = np.array(img)\n",
    "#     return data\n",
    "\n",
    "\n",
    "# def ageAtPhoto(fileName):\n",
    "#     '''\n",
    "#     get age at time of photo \n",
    "#     ageAtPhoto('full_path_to_file')\n",
    "#     10049200_1891-09-16_1958.jpg\n",
    "#     yob is 1891\n",
    "#     dtpt is 1958 \n",
    "#     '''\n",
    "#     tailPath = os.path.basename(fileName)\n",
    "#     matchObj = re.search(r'^[^_]+_([^_]+)_[^_]+$', tailPath)\n",
    "#     yob = int(matchObj.group(1)[:4])\n",
    "#     nojpg = matchObj.group(0)[:-4]\n",
    "#     dtpt = int(nojpg[-4:])\n",
    "#     currAge = abs(dtpt - yob)\n",
    "#     return currAge\n",
    "\n",
    "\n",
    "# def convertToNumpy(folder):\n",
    "#     '''\n",
    "#     get pixels and age for each image in a folder\n",
    "#     x_values, y_values = convertToNumpy(fileNames)    \n",
    "#     '''\n",
    "#     pixels = []\n",
    "#     ages = []\n",
    "#     for fileName in folder:\n",
    "#         if fileName.endswith(\".jpg\" or \".jpeg\" or \".png\"):\n",
    "#             pixels.append(np.ravel(load_image2(fileName)))\n",
    "#             ages.append(ageAtPhoto(fileName))\n",
    "#     return pixels, ages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folderName = '/Users/davidkes/classes/697/Project/wiki_crop/' \n",
    "fileNames = glob.glob(folderName +'*_resized/*.jpg')\n",
    "# only test 20 files for now\n",
    "NumberOfFileToBetested =300\n",
    "\n",
    "x_values, y_values = convertToNumpy(fileNames[:NumberOfFileToBetested])\n",
    "\n",
    "# print x_values\n",
    "# print y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16384"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_rdd = sc.parallelize(x_values).map(lambda x : x.tolist()).map(lambda x: [int(element) for element in x])\n",
    "# len(flat_rdd.take(5)[0])\n",
    "age_rdd = sc.parallelize(y_values).map(lambda x:int(x))\n",
    "# age_rdd.take(5)\n",
    "combined = flat_rdd.zip(age_rdd)\n",
    "# combined.collect()[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a DataFrame\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "penschema = StructType([\n",
    "   StructField(\"features\", ArrayType(elementType=IntegerType(),containsNull=False),True),\n",
    "   StructField(\"label\", IntegerType(),True)\n",
    "])\n",
    "dfpen1 = sqlContext.createDataFrame(combined.map(lambda x : Row(x[0][:],x[1])), penschema)\n",
    "dfpen1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "dfpen1 = dfpen1.select(list_to_vector_udf(dfpen1[\"features\"]).alias(\"features\"),'label')\n",
    "\n",
    "pendtsets = dfpen1.randomSplit([0.8, 0.2])\n",
    "pendttrain = pendtsets[0].cache()\n",
    "pendtvalid = pendtsets[1].cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(regParam=0.01, maxIter=10, fitIntercept=True)\n",
    "lrmodel = lr.fit(pendttrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[12.0,12.0,12.0,1...|   35|[7.38833377810578...|[0.00148664721115...|      51.0|\n",
      "|[25.0,25.0,25.0,2...|   26|[4.20361383406276...|[1.14284124341466...|      30.0|\n",
      "|[61.0,61.0,61.0,6...|   24|[4.09090963001608...|[1.13888916099408...|      21.0|\n",
      "|[62.0,62.0,62.0,6...|   66|[2.71120730668195...|[9.94297652392477...|      46.0|\n",
      "|[143.0,143.0,143....|   60|[13.2968108020684...|[0.00751876820522...|      31.0|\n",
      "|[165.0,165.0,165....|   40|[8.60832564226181...|[1.65832346658064...|      21.0|\n",
      "|[171.0,170.0,169....|   23|[11.9945858863914...|[0.01281040676709...|      27.0|\n",
      "|[176.0,176.0,176....|   26|[11.5420644153596...|[8.73401397146175...|      20.0|\n",
      "|[213.0,213.0,213....|   24|[8.12484575481163...|[0.00119119511604...|      20.0|\n",
      "|[215.0,221.0,219....|   47|[8.34644149871255...|[2.80485132717574...|      32.0|\n",
      "|[247.0,247.0,247....|   29|[11.5226716358781...|[0.00854926297082...|      50.0|\n",
      "|[248.0,253.0,253....|   25|[10.2053373226512...|[0.00208773683854...|      29.0|\n",
      "|[0.0,0.0,0.0,0.0,...|   32|[0.46579802246422...|[1.37667423114667...|      22.0|\n",
      "|[5.0,5.0,5.0,5.0,...|   77|[2.60588337999514...|[1.23319352102967...|      31.0|\n",
      "|[7.0,7.0,7.0,7.0,...|   36|[1.63111836284144...|[4.64615109810196...|      30.0|\n",
      "|[11.0,12.0,14.0,1...|   26|[2.38560201274946...|[1.34793650210272...|      30.0|\n",
      "|[83.0,82.0,80.0,8...|   30|[11.8996459120334...|[0.01906669132105...|      23.0|\n",
      "|[96.0,96.0,96.0,9...|   21|[11.8572401271336...|[3.22818427195996...|      30.0|\n",
      "|[120.0,120.0,119....|   24|[7.92790127112501...|[0.00141126107808...|      29.0|\n",
      "|[121.0,121.0,121....|   23|[5.73121288787272...|[0.00289092730146...|      33.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validpredict = lrmodel.transform(pendtvalid)\n",
    "validpredict.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.0242\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "bceval = MulticlassClassificationEvaluator()\n",
    "print (\"F1 = %.4f\" % bceval.evaluate(validpredict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
