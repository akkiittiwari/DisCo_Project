{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageOps \n",
    "import numpy as np \n",
    "import os \n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf = (conf.setMaster('local[*]')\n",
    "        .set('spark.executor.memory', '4G')\n",
    "        .set('spark.driver.memory', '16G')\n",
    "        .set('spark.driver.maxResultSize', '10G'))\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resizing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageResize(basename,imageName):\n",
    "    \"\"\"\n",
    "    resize image\n",
    "    basename : eg. /home/username/XYZFolder\n",
    "    image name : xyz.jpg\n",
    "    New folder in the working directory will be created with '_resized' as suffix\n",
    "    \"\"\"\n",
    "    new_width  = 128\n",
    "    new_height = 128\n",
    "    try:  \n",
    "        img = Image.open(basename+\"/\"+imageName) # image extension *.png,*.jpg\n",
    "        img = img.resize((new_width, new_height), Image.ANTIALIAS)\n",
    "        img.save(basename+'_resized/'+imageName)\n",
    "    except:\n",
    "        os.mkdir(basename+'_resized/')\n",
    "        img = Image.open(basename+\"/\"+imageName) # image extension *.png,*.jpg\n",
    "        img = img.resize((new_width, new_height), Image.ANTIALIAS)\n",
    "        img.save(basename+'_resized/'+imageName)\n",
    "\n",
    "def resizer(folderPath):\n",
    "    \"\"\"\n",
    "    to resize all files present in a folder\n",
    "    resizer('/home/username/XYZFolder')\n",
    "    \"\"\"\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(folderPath):\n",
    "        for fileName in files:\n",
    "#             try:\n",
    "                #  print os.path.join(subdir, file)\n",
    "                filepath = subdir + os.sep + fileName\n",
    "                #  print filepath\n",
    "                if filepath.endswith(\".jpg\" or \".jpeg\" or \".png\" or \".gif\"):\n",
    "                    imageResize(subdir,fileName)\n",
    "#             except:\n",
    "#                 print traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# resizer('wiki_crop/wiki_crop_new/')\n",
    "# # went to wiki_crop/wiki_crop_new/_resized/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering & Converting images to pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'wiki_crop/wiki_crop_new/_resized/'\n",
    "def filterimage(path):\n",
    "    my_sub_dir = glob.glob(path + '*.jpg')\n",
    "    for i in my_sub_dir:\n",
    "        if os.path.getsize(i) < 1000:\n",
    "            # print(path + str(i) + '/' + str(j))\n",
    "            os.remove(i)\n",
    "\n",
    "#filter images that are corrupted\n",
    "# filterimage(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image2(infilename) :\n",
    "    '''  \n",
    "    convert image file to pixels\n",
    "    load_image2('fileName')\n",
    "    '''\n",
    "    img = Image.open(infilename).convert('L')\n",
    "    data = np.array(img)\n",
    "    return data\n",
    "\n",
    "\n",
    "def ageAtPhoto(fileName):\n",
    "    '''\n",
    "    get age at time of photo\n",
    "    ageAtPhoto('full_path_to_file')\n",
    "    10049200_1891-09-16_1958.jpg\n",
    "    yob is 1891\n",
    "    dtpt is 1958\n",
    "    '''\n",
    "    basename = fileName.split('/')[-1].split('_')\n",
    "    birth = int(basename[1].split('-')[0])\n",
    "    today = int(basename[2].split('.')[0])\n",
    "    currAge = abs(today - birth)\n",
    "    return currAge\n",
    "\n",
    "\n",
    "\n",
    "def convertToNumpy(folder):\n",
    "    '''\n",
    "    get pixels and age for each image in a folder\n",
    "    x_values, y_values = convertToNumpy(fileNames)    \n",
    "    '''\n",
    "    pixels = []\n",
    "    ages = []\n",
    "    filename =[]\n",
    "    for fileName in folder:\n",
    "        if fileName.endswith(\".jpg\" or \".jpeg\" or \".png\"):\n",
    "            age = ageAtPhoto(fileName)\n",
    "            if (age<100 and age>0):\n",
    "                img_px = np.ravel(load_image2(fileName))\n",
    "                pixels.append(img_px)\n",
    "                ages.append(age)\n",
    "                filename.append(fileName.split('/')[-1])\n",
    "    return pixels, ages,filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderName = 'wiki_crop/wiki_crop_new/_resized/' \n",
    "fileNames = glob.glob(folderName +'*.jpg')\n",
    "# only test 20 files for now\n",
    "NumberOfFileToTrained =100000\n",
    "\n",
    "x_values, y_values,filename = convertToNumpy(fileNames[:NumberOfFileToTrained])\n",
    "\n",
    "# print x_values\n",
    "# print y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1975"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting the data on RDD and converting to DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_rdd = sc.parallelize(x_values).map(lambda x : x.tolist()).map(lambda x: [int(element) for element in x])\n",
    "# len(flat_rdd.take(5)[0])\n",
    "age_rdd = sc.parallelize(y_values).map(lambda x:int(x))\n",
    "# age_rdd.take(5)\n",
    "f_name = sc.parallelize(filename)\n",
    "combined = flat_rdd.zip(age_rdd).zip(f_name)\n",
    "# combined.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "combined.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|            features|label|              f_name|\n",
      "+--------------------+-----+--------------------+\n",
      "|[67, 67, 67, 65, ...|   21|31843216_1990-06-...|\n",
      "|[139, 140, 142, 1...|   46|1000001952958_196...|\n",
      "|[0, 0, 0, 0, 0, 0...|   98|12872267_1910-10-...|\n",
      "|[224, 223, 216, 2...|   19|43481633_1995-11-...|\n",
      "|[220, 219, 218, 2...|   52|1000001671401_195...|\n",
      "|[3, 3, 3, 3, 3, 3...|   59|100000185210_1944...|\n",
      "|[21, 19, 18, 21, ...|   27|1000007243240_197...|\n",
      "|[196, 196, 196, 1...|   31|10000022821347_19...|\n",
      "|[35, 35, 35, 35, ...|   61|10000029563647_19...|\n",
      "|[23, 23, 23, 23, ...|   28|1000003156912_198...|\n",
      "|[61, 60, 60, 62, ...|   39|10000026256606_19...|\n",
      "|[83, 71, 57, 54, ...|   34|610477_1976-04-20...|\n",
      "|[123, 123, 123, 1...|   45|30948829_1963-03-...|\n",
      "|[204, 204, 204, 2...|   34|10000012277158_19...|\n",
      "|[0, 0, 0, 0, 0, 0...|   27|1000001924882_194...|\n",
      "|[252, 252, 252, 2...|   24|10000015230998_19...|\n",
      "|[195, 200, 205, 2...|   63|10000028738868_19...|\n",
      "|[138, 137, 142, 1...|   23|19187533_1960-09-...|\n",
      "|[103, 103, 102, 1...|   28|1000001462396_198...|\n",
      "|[70, 69, 70, 70, ...|   30|680054_1980-04-22...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create a DataFrame\n",
    "imageschema = StructType([\n",
    "   StructField(\"features\", ArrayType(elementType=IntegerType(),containsNull=False),True),\n",
    "   StructField(\"label\", IntegerType(),True),\n",
    "   StructField(\"f_name\", StringType(),True)\n",
    "])\n",
    "df = sqlContext.createDataFrame(combined.map(lambda x : Row(x[0][0][:],x[0][1],x[1])), imageschema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = false)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- f_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download df in json format\n",
    "# df.write.format('json').save('project/dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[67.0,67.0,67.0,6...|   21|\n",
      "|[139.0,140.0,142....|   46|\n",
      "|[0.0,0.0,0.0,0.0,...|   98|\n",
      "|[224.0,223.0,216....|   19|\n",
      "|[220.0,219.0,218....|   52|\n",
      "|[3.0,3.0,3.0,3.0,...|   59|\n",
      "|[21.0,19.0,18.0,2...|   27|\n",
      "|[196.0,196.0,196....|   31|\n",
      "|[35.0,35.0,35.0,3...|   61|\n",
      "|[23.0,23.0,23.0,2...|   28|\n",
      "|[61.0,60.0,60.0,6...|   39|\n",
      "|[83.0,71.0,57.0,5...|   34|\n",
      "|[123.0,123.0,123....|   45|\n",
      "|[204.0,204.0,204....|   34|\n",
      "|[0.0,0.0,0.0,0.0,...|   27|\n",
      "|[252.0,252.0,252....|   24|\n",
      "|[195.0,200.0,205....|   63|\n",
      "|[138.0,137.0,142....|   23|\n",
      "|[103.0,103.0,102....|   28|\n",
      "|[70.0,69.0,70.0,7...|   30|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write df to MongoDB\n",
    "df.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"append\").save()\n",
    "\n",
    "# Read df from MongoDB\n",
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/msan697.images\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the df to train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "df = df.select(list_to_vector_udf(df[\"features\"]).alias(\"features\"),'label')\n",
    "\n",
    "dataset = df.randomSplit([0.8, 0.2])\n",
    "train = dataset[0].cache()\n",
    "test = dataset[1].cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.show()\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=10, fitIntercept=True)\n",
    "lrmodel = lr.fit(train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[0.0,0.0,0.0,0.0,...|   22|[0.47553352010982...|[0.00151244331494...|      22.0|\n",
      "|[0.0,0.0,0.0,0.0,...|   27|[0.47553352010982...|[0.00151244331494...|      22.0|\n",
      "|[13.0,21.0,31.0,3...|   49|[0.86286628415266...|[6.31712801656277...|      34.0|\n",
      "|[63.0,63.0,63.0,6...|   28|[2.25797846458182...|[1.22932906023510...|      29.0|\n",
      "|[78.0,79.0,78.0,7...|   33|[1.63863691211937...|[1.03280974741870...|      31.0|\n",
      "|[79.0,89.0,92.0,8...|   35|[1.58516471067865...|[1.94723527919132...|      26.0|\n",
      "|[83.0,83.0,83.0,8...|   32|[1.84280598911004...|[7.18916993737185...|      31.0|\n",
      "|[97.0,99.0,100.0,...|   33|[1.50851905888919...|[4.22414611451184...|      31.0|\n",
      "|[103.0,103.0,102....|   28|[1.84717808615284...|[1.94383488453167...|      28.0|\n",
      "|[196.0,196.0,196....|   31|[2.42418962783622...|[9.68526512493888...|      26.0|\n",
      "|[0.0,0.0,0.0,0.0,...|   26|[0.47553352010982...|[0.00151244331494...|      22.0|\n",
      "|[0.0,0.0,0.0,0.0,...|  108|[0.47553352010982...|[0.00151244331494...|      22.0|\n",
      "|[3.0,3.0,3.0,3.0,...|   24|[1.10095026387872...|[3.41984578066013...|      29.0|\n",
      "|[39.0,39.0,39.0,3...|   81|[1.71415325627850...|[1.60917077839793...|      29.0|\n",
      "|[64.0,61.0,56.0,5...|   20|[2.06240288997105...|[3.50943023233356...|      27.0|\n",
      "|[79.0,82.0,80.0,8...|   24|[2.52192138157612...|[5.11738572467362...|      82.0|\n",
      "|[103.0,102.0,78.0...|   36|[1.62428509926365...|[4.74549569187305...|      25.0|\n",
      "|[105.0,95.0,88.0,...|   68|[1.51559458959588...|[2.95923516970875...|      34.0|\n",
      "|[124.0,127.0,137....|   28|[2.79928748646689...|[3.18187588629871...|      25.0|\n",
      "|[184.0,179.0,180....|   45|[2.11551259463384...|[1.41450767589865...|      43.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validpredict = lrmodel.transform(test)\n",
    "validpredict.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
